---
title: 统计学习方法——感知机
tags: 机器学习
date: 2018-10-30 09:06:26
mathjax: true
---


感知机是二类分类的线性分类模型，其输入为实例的特征向量，输出为实例的类别，取`+1`和`-1`二值。感知机对应于输入空间（特征空间）中将实例划分为正负两类的分离超平面，属于判别模型。感知机学习旨在求出将训练数据进行线性划分的分离超平面，为此，导入基于误分类的损失函数，利用梯度下降法对损失函数进行极小化，求得感知机模型。感知机学习算法具有简单而易于实现的优点，分为原始形式和对偶形式。感知机预测是用学习得到的感知机模型对新的输入实例进行分类。感知机1957年由Rosenblatt提出，是神经网络与支持向量机的基础。

<!-- more -->

## 感知机模型
**感知机的定义**：假设输入空间(特征空间)是$X\subseteq R^n$，输出空间是$Y=\\{+1,-1\\}$.输入$x\in X$ 表示实例的特征向量，对应于输入空间的点；输出$y\in Y$表示实例的类别.由输入空间到输出空间的如下函数：
$$f(x)=sign(w \cdot x+b)\tag{1}$$
称为感知机.其中，$w$和$b$为感知机模型参数，$w\in R^n$叫做权值(weight)或者权值向量(weight vector)，$b\in R$叫做偏置(bias),$w\cdot x$表示$w$和$x$的内积.$sign$是符号函数,即:
$$sign(x)=
\begin{cases}
+1& x\geq 0\\\\
-1& x\lt 0
\end{cases}\tag{2}$$
感知机是一种线性分类模型，属于判别模型.感知机模型的假设空间是定义在特征空间中的所有线性分类模型(linear classification model)或者线性分类器(linear classifier),即函数集合$\\{f|f(x)=w\cdot x+b\\}$.

感知机有如下几何解释:线性方程
$$w\cdot x+b=0\tag{3}$$
对应于特征空间$R^n$中的一个超平面，其中$w$是超平面的法向量，b是超平面的截距.这个超平面将特征空间划分为两个部分.位于两部分的点(特征向量)分别被分为正、负两类.因此，超平面$S$称为分离超平面(separating hyperplane).

感知机学习，由训练数据集(实例的特征向量及类别)$$T=\\{(x_1,y_1),(x_2,y_2),\cdots,(x_N,y_N)\\}$$其中，$x_i\in X = R^n$,$y_i\in Y = \\{+1,-1\\},i=1,2,\cdots,N$,求得感知机模型，即求得模型参数$w$,$b$.感知机预测，通过学习得到的感知机模型，对于新的输入实例给出其对应的输出类别.

## 感知机学习策略
### 数据集的线性可分性
给定一个数据集$$T=\\{(x_1,y_1),(x_2,y_2),\cdots,(x_N,y_N)\\}$$其中，$x_i\in X = R^n$,$y_i\in Y = \\{+1,-1\\},i=1,2,\cdots,N$,如果存在某个超平面$S$
$$w\cdot x+b=0$$能够将数据集的正实例点和负实例点完全正确地划分到超平面的两侧，即对所有$y_i=+1$的实例$i$,有$w\cdot x+b\gt 0$,对所有$y_i=-1$的实例$i$,有$w\cdot x+b\lt 0$,则称数据集$T$为线性可分数据集(linearly separable data set);否则称数据集$T$线性不可分.

### 感知机学习策略
假设训练数据集是线性可分的，感知机学习的目标是求得一个能够将训练集正实例点和负实例点完全正确分开的分离超平面.为了找出这样的超平面，即确定感知机模型参数$w$,$b$，需要确定一个学习策略，即定义损失函数并将损失函数极小化.

感知机使用的损失函数是误分类点到超平面$S$的总距离.

输入空间$R^n$中任一点$x_0$到超平面$S$的距离:$$\frac{1}{\parallel w \parallel}|w\cdot x_0+b|$$这里，$\parallel w \parallel$是$w$的$L_2$范数.

对于误分类的数据$(x_i,y_i)$来说，$$-y_i(w\cdot x+b)>0$$成立.因此，误分类点$x_i$到超平面$S$的距离是$$-\frac{1}{\parallel w \parallel}y_i(w\cdot x_i+b)$$

这样，假设超平面$S$的误分类点集合为$M$,那么所有误分类点到超平面$S$的总距离为$$-\frac{1}{\parallel w \parallel}\sum\limits_{x_i\in M} y_i(w\cdot x_i+b)$$
不考虑$\frac{1}{\parallel w \parallel}$,就得到感知机学习的损失函数.

给定训练数据集$$T=\\{(x_1,y_1),(x_2,y_2),\cdots,(x_N,y_N)\\}$$其中，$x_i\in X = R^n$,$y_i\in Y = \\{+1,-1\\},i=1,2,\cdots,N$.感知机$sign(w\cdot x+b)$学习的损失函数定义为$$L(w,b)=-\sum\limits_{x_i\in M} y_i(w\cdot x_i+b)\tag{4}$$其中$M$是误分类点的集合.这个损失函数就是感知机学习的经验风险函数.

感知机学习的策略是在假设空间中选取使损失函数式最小的模型参数$w$,$b$，即感知机模型.
## 感知机学习算法
感知机学习问题转化为求解损失函数式的最优化问题，最优化的方法是随机梯度下降法.感知机学习算法分为原始形式和对偶形式。

### 感知机学习算法的原始形式
感知机学习算法是误分类驱动的，具体采用随机梯度下降法(stochastic gradient descent).首先，任意选取一个超平面$w_0$,$b_0$,然后用梯度下降法不断地极小化目标函数.极小化过程中不是一次使$M$中所有误分类点的梯度下降，而是一次随机选取一个误分类点使其梯度下降.

假设误分类点集合$M$是固定的，那么损失函数$L(w,b)$的梯度是：$$\nabla_w L(w,b)=-\sum\limits_{x_i\in M}y_i x_i$$$$\nabla_b L(w,b)=-\sum\limits_{x_i\in M}y_i$$

随机选取一个误分类点$(x_i,y_i)$,对$w$,$b$进行更新:
$$w\leftarrow w+\eta y_ix_i\tag{5}$$$$b\leftarrow b+\eta y_i\tag{6}$$式中$\eta(0\lt\eta\leq1)$是步长，在统计学习中又称为学习率(learning rate).这样，通过迭代可以期待损失函数$L(w,b)$不断减小，直到为0.

**感知机学习算法的原始形式**

输入：训练数据集$$T=\\{(x_1,y_1),(x_2,y_2),\cdots,(x_N,y_N)\\}$$其中，$x_i\in X = R^n$,$y_i\in Y = \\{+1,-1\\},i=1,2,\cdots,N$;

学习率$\eta(0\lt\eta\leq1)$;

输出:$w$,$b$;感知机模型$f(x)=sign(w\cdot x+b)$.

1. 选取初值$w_0$,$b_0$
2. 在训练集中选取数据$(x_i,y_i)$
3. 如果$y_i(w\cdot x_i+b)\leq0$$$w\leftarrow w+\eta y_ix_i$$$$b\leftarrow b+\eta y_i$$
4. 转至2，直至训练集中没有误分类点.

这种学习算法直观上有如下解释：当一个实例点被误分类，即位于分离超平面的错误一侧时，则调整$w$,$b$的值，使分离超平面向该误分类点的一侧移动，以减少该误分类点与超平面间的距离，直至超平面越过该误分类点使其被正确分类.